{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from preprocess import *\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "WORD_IMG_SHAPE = (64, 64 * 4)\n",
    "LATENT_SIZE = np.prod(WORD_IMG_SHAPE)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = nn.Sequential(\n",
    "    # in: latent_size x 1 x 1\n",
    "\n",
    "    nn.ConvTranspose2d(LATENT_SIZE, 1024, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    nn.BatchNorm2d(1024),\n",
    "    nn.ReLU(True),\n",
    "    # out: 512 x 4 x 4\n",
    "\n",
    "    nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(True),\n",
    "    # out: 256 x 8 x 8\n",
    "\n",
    "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(True),\n",
    "    # out: 128 x 16 x 16\n",
    "\n",
    "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(True),\n",
    "    # out: 64 x 32 x 32\n",
    "    \n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(True),\n",
    "    # out: 64 x 32 x 32\n",
    "\n",
    "    nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.Tanh(),\n",
    "    # out: 1 x 64 x 64\n",
    "    \n",
    "    Reshape('0', 1, '2*3')\n",
    ")\n",
    "\n",
    "# generator = nn.Sequential(\n",
    "#     # in: latent_size x 1 x 1\n",
    "\n",
    "#     nn.ConvTranspose2d(LATENT_SIZE, 2048, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "#     nn.BatchNorm2d(2048),\n",
    "#     nn.ELU(True),\n",
    "#     # out: 2048 x 4 x 4\n",
    "\n",
    "#     nn.ConvTranspose2d(2048, 1024, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "#     nn.BatchNorm2d(1024),\n",
    "#     nn.ELU(True),\n",
    "#     # out: 1024 x 8 x 8\n",
    "\n",
    "#     nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ELU(True),\n",
    "#     # out: 512 x 16 x 16\n",
    "    \n",
    "#     nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "#     nn.MaxPool2d(kernel_size=4, stride=2, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ELU(True),\n",
    "#     # out: 512 x 16 x 16\n",
    "\n",
    "#     nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ELU(True),\n",
    "#     # out: 256 x 32 x 32\n",
    "    \n",
    "#     nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "#     nn.MaxPool2d(kernel_size=4, stride=2, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ELU(True),\n",
    "#     # out: 256 x 32 x 32\n",
    "    \n",
    "#     nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ELU(True),\n",
    "#     # out: 128 x 64 x 64\n",
    "    \n",
    "#     nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.MaxPool2d(kernel_size=4, stride=2, padding=1),\n",
    "#     nn.ELU(True),\n",
    "#     # out: 64 x 64 x 64\n",
    "\n",
    "#     nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "#     nn.Tanh(),\n",
    "#     # out: 1 x 64 x 64\n",
    "    \n",
    "#     Reshape('0', 1, '2*3')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_model = load_char_restoration_model()\n",
    "# # print(char_model.named_parameters)\n",
    "# # print(generator.named_parameters)\n",
    "# # generator.no_grad()\n",
    "# # char_model.no_grad()\n",
    "# generator = copy_weights(char_model, generator)\n",
    "# generator.load_state_dict(torch.load('./trained/word/full/33/generator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = 'generated'\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "def save_samples(index, latent_tensors, show=True, num_samples=3*8):\n",
    "    fake_images = generator(latent_tensors)\n",
    "    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n",
    "    save_image(fake_images.reshape((num_samples, 1, *WORD_IMG_SHAPE)), os.path.join(sample_dir, fake_fname), nrow=3, pad_value=1)\n",
    "    print('Saving', fake_fname)\n",
    "    if show:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=12).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_generator(opt_g, base_imgs, crpt_imgs):\n",
    "    batch_size = base_imgs.shape[0]\n",
    "    # Clear generator gradients\n",
    "    opt_g.zero_grad()\n",
    "    \n",
    "    # Generate fake images\n",
    "    # latent = torch.randn(batch_size, latent_size, 1, 1)\n",
    "    # corrupted, base, labels = next(cwg)\n",
    "    # print(np.shape(crpt_imgs))\n",
    "    # print(np.shape(crpt_imgs[0]))\n",
    "    # print(type(crpt_imgs))\n",
    "    # print(crpt_imgs.reshape((BATCH_SIZE, LATENT_SIZE, 1, 1)).shape)\n",
    "    \n",
    "    rest_imgs = generator(crpt_imgs.reshape((batch_size, LATENT_SIZE, 1, 1)))\n",
    "    # rest_imgs = generator(base_imgs.reshape((batch_size, LATENT_SIZE, 1, 1)))\n",
    "   \n",
    "    # print(f\"rest img shape: {rest_imgs.shape}\")\n",
    "    # Try to fool the discriminator\n",
    "    # fool_preds = discriminator(fake_images)\n",
    "    # fool_targets = torch.ones(batch_size, 1)\n",
    "    # fool_loss = F.mse_loss(fool_preds, fool_targets)\n",
    "    \n",
    "    sim_loss = F.mse_loss(rest_imgs / 2, (base_imgs / 2).reshape((batch_size, 1, LATENT_SIZE)))\n",
    "    # print(f\"loss shape: {sim_loss.shape}\")\n",
    "    # black_loss = F.l1_loss(torch.zeros(rest_imgs.shape), (-rest_imgs + 1) / 2)\n",
    "    # print(base_imgs.shape)\n",
    "    # print(rest_imgs.shape)\n",
    "    ink_loss = F.mse_loss(torch.zeros(rest_imgs.shape), \n",
    "                          (img_subtract(base_imgs.reshape(rest_imgs.shape), rest_imgs) + 1) / 2)\n",
    "    \n",
    "    # Update generator weights\n",
    "    # loss = 2.5 * fool_loss + 150 * sim_loss\n",
    "    loss = 50 * sim_loss + 150 * ink_loss\n",
    "    loss.backward()\n",
    "    opt_g.step()\n",
    "    \n",
    "    print(f\"generator loss: {(torch.mean(loss).item())}\")\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 3*8\n",
    "\n",
    "data_loader = DataLoader(load_dataset('char', equal_shapes=False), shuffle=True, num_workers=0, pin_memory=True)\n",
    "cwg = CorruptWordGen(data_loader, batch_size=num_examples, img_shape=WORD_IMG_SHAPE)\n",
    "\n",
    "fixed_base, fixed_corrupted, fixed_labels = next(cwg)\n",
    "fixed_corrupted = fixed_corrupted.reshape((num_examples, LATENT_SIZE, 1, 1))\n",
    "\n",
    "def fit(epochs, lr, start_idx=1):\n",
    "  torch.cuda.empty_cache()\n",
    "    \n",
    "  # Losses & scores\n",
    "  losses_g = []\n",
    "  losses_d = []\n",
    "  real_scores = []\n",
    "  fake_scores = []\n",
    "  gen_scores = []\n",
    "  \n",
    "  # Create optimizers\n",
    "  # opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "  opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "  \n",
    "  for epoch in range(epochs):\n",
    "    data_loader = DataLoader(load_dataset('char', equal_shapes=False), shuffle=True, num_workers=0, pin_memory=True)\n",
    "    cwg = CorruptWordGen(data_loader, batch_size=BATCH_SIZE, img_shape=WORD_IMG_SHAPE)\n",
    "    \n",
    "    for base_imgs, crpt_imgs, labels in tqdm(cwg):\n",
    "      \n",
    "      # # Train discriminator\n",
    "      # loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n",
    "      # # Train generator\n",
    "      loss_g = train_generator(opt_g, base_imgs, crpt_imgs)\n",
    "        \n",
    "    # Record losses & scores\n",
    "    losses_g.append(loss_g)\n",
    "    # losses_d.append(loss_d)\n",
    "    # real_scores.append(real_score)\n",
    "    # fake_scores.append(fake_score)\n",
    "    \n",
    "    # Log losses & scores (last batch)\n",
    "    print(\"Epoch [{}/{}], loss_g: {:.4f}\".format(\n",
    "      epoch+1, epochs, loss_g))\n",
    "\n",
    "    # Save generated images\n",
    "    save_samples(epoch+start_idx, fixed_corrupted, show=False, num_samples = num_examples)\n",
    "    \n",
    "    torch.save(generator.state_dict(), \"./trained/word/generator\")\n",
    "    torch.save(opt_g.state_dict(), './trained/word/opt')\n",
    "    try:\n",
    "      torch.save(generator.state_dict(), f\"./trained/word/{epoch}/generator\")\n",
    "      torch.save(opt_g.state_dict(), f'./trained/word/{epoch}/opt')\n",
    "    except:\n",
    "      pass\n",
    "  \n",
    "  return losses_g, losses_d, real_scores, fake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21, 4, 21, 4], [9, 19, 23, 17, 1, 17, 4, 16, 2], [4, 23, 1, 2, 18, 11, 0, 11], [1, 2, 7, 6, 24, 6, 23], [23, 24, 21], [9, 21], [11, 19, 13, 19], [1, 16, 5, 13, 22, 5], [18, 4, 16, 5, 6, 4, 4], [2, 12], [5, 1, 21, 19, 6], [23, 12, 1, 2, 11, 18], [1, 11, 13, 3, 19], [2, 18, 10, 2, 1], [0, 20, 18], [21, 12, 16, 5, 1], [18, 5], [23, 23], [4, 23], [23, 20, 0, 16, 12, 0], [12, 9, 13, 11], [23, 7, 4, 7, 11], [13, 18, 18, 19], [6, 3, 24, 14, 6]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(torch.cat(fixed_base).shape)\n",
    "# print(np.shape(fixed_base[0]))\n",
    "# print(np.shape(fixed_corrupted[0]))\n",
    "save_image(fixed_base.reshape((num_examples, 1, *WORD_IMG_SHAPE)), os.path.join('generated', \"base.png\"), nrow=3, pad_value=1)\n",
    "save_image(fixed_corrupted.reshape((num_examples, 1, *WORD_IMG_SHAPE)), os.path.join('generated', \"corrupted.png\"), nrow=3, pad_value=1)\n",
    "print(fixed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[24, 1, 64, 256]' is invalid for input of size 221184",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12996/1730818556.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msave_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_corrupted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12996/2522265655.py\u001b[0m in \u001b[0;36msave_samples\u001b[1;34m(index, latent_tensors, show, num_samples)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfake_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfake_fname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'generated-images-{0:0=4d}.png'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0msave_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mWORD_IMG_SHAPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_fname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Saving'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_fname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[24, 1, 64, 256]' is invalid for input of size 221184"
     ]
    }
   ],
   "source": [
    "save_samples(0, fixed_corrupted, show=False, num_samples = num_examples)\n",
    "history = fit(100, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
