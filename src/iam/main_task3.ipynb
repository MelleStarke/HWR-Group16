{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:42:26.164178Z",
     "iopub.status.busy": "2022-06-03T06:42:26.163592Z",
     "iopub.status.idle": "2022-06-03T06:42:26.183248Z",
     "shell.execute_reply": "2022-06-03T06:42:26.182552Z",
     "shell.execute_reply.started": "2022-06-03T06:42:26.164094Z"
    },
    "executionInfo": {
     "elapsed": 28838,
     "status": "ok",
     "timestamp": 1654171228963,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "z7pED5fNCjtx",
    "outputId": "91b4aae2-70f9-42b1-aa78-7c657cd40c9f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Handwriting Recognition Project - Task 3\n",
    "# Group 16\n",
    "# 03-06-2022\n",
    "#\n",
    "# This code was largely based on the example presented in https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_native_PyTorch.ipynb\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'D:\\Documenten\\Universiteit_master\\HR\\task_3\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install the Hugging Face TROCR base model\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:42:26.199388Z",
     "iopub.status.busy": "2022-06-03T06:42:26.198756Z",
     "iopub.status.idle": "2022-06-03T06:42:38.629086Z",
     "shell.execute_reply": "2022-06-03T06:42:38.628088Z",
     "shell.execute_reply.started": "2022-06-03T06:42:26.199348Z"
    },
    "executionInfo": {
     "elapsed": 7571,
     "status": "ok",
     "timestamp": 1654171236951,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "12Yr-IrlCjt8",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'D:\\Documenten\\Universiteit_master\\HR\\task_3\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install jiwer\n",
    "!pip install -q datasets jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:42:38.632263Z",
     "iopub.status.busy": "2022-06-03T06:42:38.631529Z",
     "iopub.status.idle": "2022-06-03T06:42:38.657377Z",
     "shell.execute_reply": "2022-06-03T06:42:38.656626Z",
     "shell.execute_reply.started": "2022-06-03T06:42:38.632222Z"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1654171236952,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "tAFikWHrCjt-",
    "outputId": "3fcb2cf1-5b62-4882-aa4b-6e4ce9283b8a",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import pandas to allow working with data frames\n",
    "import pandas as pd\n",
    "\n",
    "# Make a data frame of the .txt data\n",
    "TRAIN_DATA_DIR = \"data/\"\n",
    "raw_txt_df = pd.read_fwf(TRAIN_DATA_DIR + 'iam_lines_gt.txt', header=None)\n",
    "first_column = raw_txt_df.iloc[::2]\n",
    "second_column = raw_txt_df.iloc[1::2]\n",
    "txt_df = pd.concat([first_column.reset_index(drop=True), second_column.reset_index(drop=True)], axis=1)\n",
    "txt_df = pd.DataFrame(txt_df)\n",
    "txt_df = txt_df.set_axis(['file_name', 'text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:42:38.659803Z",
     "iopub.status.busy": "2022-06-03T06:42:38.659411Z",
     "iopub.status.idle": "2022-06-03T06:42:39.172382Z",
     "shell.execute_reply": "2022-06-03T06:42:39.171617Z",
     "shell.execute_reply.started": "2022-06-03T06:42:38.659762Z"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1654171236952,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "mVa91Vpxz2XM",
    "outputId": "dc02fce5-a73a-460e-c0a3-207f33f7c2e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import a train_test_split function to allow for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use a 80-20 train-validation split\n",
    "train_txt_df, val_txt_df = train_test_split(txt_df, test_size=0.2)\n",
    "train_txt_df.reset_index(drop=True, inplace=True)\n",
    "val_txt_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:42:39.175791Z",
     "iopub.status.busy": "2022-06-03T06:42:39.175493Z",
     "iopub.status.idle": "2022-06-03T06:42:39.950262Z",
     "shell.execute_reply": "2022-06-03T06:42:39.949448Z",
     "shell.execute_reply.started": "2022-06-03T06:42:39.175766Z"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1654171236954,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "HGJyblEXL_nv",
    "outputId": "f9cb9acd-eb57-4952-8652-037b399d8f68",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMG_TRAIN_DATA_DIR = 'data/img/'\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "# Iterate over the IAM data, generate binary images and show these\n",
    "# While the images are not used, for know, this provides an illustration\n",
    "for filename in os.listdir(IMG_TRAIN_DATA_DIR):\n",
    "    # Get an image\n",
    "    f = os.path.join(IMG_TRAIN_DATA_DIR, filename)\n",
    "    img = cv2.imread(f)\n",
    "    \n",
    "    # Present 10 images in total\n",
    "    cnt = cnt + 1\n",
    "    if cnt == 10:\n",
    "      break\n",
    "\n",
    "    # Threshold the image\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    (thresh, threshold_img) = cv2.threshold(gray_img, 200, 255, cv2.THRESH_BINARY)\n",
    "    window_name = \"img\"\n",
    "    \n",
    "    # Show the resulting binary image\n",
    "    plt.imshow(threshold_img, cmap='gray')\n",
    "    plt.imshow(gray_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:42:39.952078Z",
     "iopub.status.busy": "2022-06-03T06:42:39.951714Z",
     "iopub.status.idle": "2022-06-03T06:43:11.701115Z",
     "shell.execute_reply": "2022-06-03T06:43:11.700168Z",
     "shell.execute_reply.started": "2022-06-03T06:42:39.952043Z"
    },
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1654171237799,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "Mjp-jT0PbXHe",
    "outputId": "5a00cc29-04bd-410e-93b6-e1a5519ea8ee",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irisk\\AppData\\Local\\Temp\\ipykernel_32608\\2892363653.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filename['file_name'] = name\n",
      "C:\\Users\\irisk\\AppData\\Local\\Temp\\ipykernel_32608\\2892363653.py:46: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_train_txt_df = new_train_txt_df.append([df_filename],ignore_index=True)\n",
      "C:\\Users\\irisk\\AppData\\Local\\Temp\\ipykernel_32608\\2892363653.py:49: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_val_txt_df = new_val_txt_df.append([df_filename],ignore_index=True)\n",
      "C:\\Users\\irisk\\AppData\\Local\\Temp\\ipykernel_32608\\2892363653.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filename['file_name'] = name\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "# from keras.preprocessing.image import img_to_array\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Create an Image Data Generator for augmentation\n",
    "# Rotate, zoom and shear the images\n",
    "augmentator = ImageDataGenerator(\n",
    "\trotation_range = 0.5,\n",
    "\tshear_range = 0.5,\n",
    "\tfill_mode = \"constant\",\n",
    "    cval = 255)\n",
    "\n",
    "# Create new data frames in which the augmented image names are included\n",
    "new_train_txt_df = train_txt_df.copy()\n",
    "new_val_txt_df = val_txt_df.copy()\n",
    "\n",
    "AUGMENT_DATA = False\n",
    "\n",
    "if AUGMENT_DATA:\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    # Loop through all images\n",
    "    for filename in os.listdir(IMG_TRAIN_DATA_DIR):\n",
    "        cnt = cnt + 1\n",
    "        \n",
    "        # Get the image\n",
    "        f = os.path.join(IMG_TRAIN_DATA_DIR, filename)\n",
    "        img = cv2.imread(f)  \n",
    "        \n",
    "        # Reshape the image so that it can be used\n",
    "        x = np.array(img)\n",
    "        x = x.reshape((1, ) + x.shape)  \n",
    "        \n",
    "        # Create the folder in which the augmented images are saved\n",
    "        aug_image_folder = \"data/img_aug/\"\n",
    "        \n",
    "        i = 0\n",
    "        # Generate 5 augmented images per image\n",
    "        for batch in augmentator.flow(x, batch_size = 1):\n",
    "            # Save the augmented image\n",
    "            name = str(i) + \"_\" + filename\n",
    "            cv2.imwrite(f'{aug_image_folder}/{name}',batch[0,:,:,:])\n",
    "            \n",
    "            # Add the augmented image name to a txt dataframe\n",
    "            df_filename = new_train_txt_df[new_train_txt_df['file_name'] == filename]\n",
    "            df_filename['file_name'] = name\n",
    "            new_train_txt_df = new_train_txt_df.append([df_filename],ignore_index=True)\n",
    "            df_filename = new_val_txt_df[new_val_txt_df['file_name'] == filename]\n",
    "            df_filename['file_name'] = name\n",
    "            new_val_txt_df = new_val_txt_df.append([df_filename],ignore_index=True)\n",
    "            \n",
    "            i += 1\n",
    "            if i > 5: \n",
    "                # Remove the original image from the txt dataframes\n",
    "                new_train_txt_df = new_train_txt_df.drop(new_train_txt_df[new_train_txt_df['file_name'] == filename].index)\n",
    "                new_val_txt_df = new_val_txt_df.drop(new_val_txt_df[new_val_txt_df['file_name'] == filename].index)\n",
    "                break\n",
    "    new_train_txt_df.reset_index(drop=True, inplace=True)\n",
    "    new_val_txt_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:43:11.703122Z",
     "iopub.status.busy": "2022-06-03T06:43:11.702367Z",
     "iopub.status.idle": "2022-06-03T06:43:13.405530Z",
     "shell.execute_reply": "2022-06-03T06:43:13.404630Z",
     "shell.execute_reply.started": "2022-06-03T06:43:11.703081Z"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1654171237800,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "1rCBW7k7CjuB",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Create a class for the IAM data (largely based on https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_native_PyTorch.ipynb)\n",
    "class IAMDataset(Dataset):\n",
    "    \n",
    "    # Initialise \n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    # Return the length\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    # Get an item from the data\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the file name and corresponding text\n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "\n",
    "        # Get the image\n",
    "        image = cv2.imread(self.root_dir + file_name)\n",
    "\n",
    "        # Create a binary image from the original image (to get rid of background noise)\n",
    "        (thresh, threshold_img) = cv2.threshold(image, 200, 255, cv2.THRESH_BINARY)\n",
    "        threshold_img = np.img_to_array(threshold_img)\n",
    "\n",
    "        # Get the pixel values\n",
    "        pixel_values = self.processor(threshold_img, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        # Get labels by encoding the text\n",
    "        labels = self.processor.tokenizer(text,\n",
    "                                          padding=\"max_length\",\n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        \n",
    "        # Ensure that the PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:43:13.407352Z",
     "iopub.status.busy": "2022-06-03T06:43:13.406969Z",
     "iopub.status.idle": "2022-06-03T06:43:19.571904Z",
     "shell.execute_reply": "2022-06-03T06:43:19.571033Z",
     "shell.execute_reply.started": "2022-06-03T06:43:13.407314Z"
    },
    "executionInfo": {
     "elapsed": 5306,
     "status": "ok",
     "timestamp": 1654171243097,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "nYWQrl_X21ab",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "# Use the base TROCR model as a processor\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# Create IAM datasets using the IAMDataset class\n",
    "train_dataset = IAMDataset(root_dir='data/img_aug/',\n",
    "                           df=new_train_txt_df,\n",
    "                           processor=processor)\n",
    "val_dataset = IAMDataset(root_dir='data/img_aug/',\n",
    "                           df=new_val_txt_df,\n",
    "                           processor=processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:43:19.575857Z",
     "iopub.status.busy": "2022-06-03T06:43:19.574362Z",
     "iopub.status.idle": "2022-06-03T06:43:19.580777Z",
     "shell.execute_reply": "2022-06-03T06:43:19.579872Z",
     "shell.execute_reply.started": "2022-06-03T06:43:19.575812Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1654171243098,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "_UMCnePUCjuF",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create data loaders for the training and validation data sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:43:19.582411Z",
     "iopub.status.busy": "2022-06-03T06:43:19.582033Z",
     "iopub.status.idle": "2022-06-03T06:44:44.993140Z",
     "shell.execute_reply": "2022-06-03T06:44:44.992366Z",
     "shell.execute_reply.started": "2022-06-03T06:43:19.582376Z"
    },
    "executionInfo": {
     "elapsed": 11112,
     "status": "error",
     "timestamp": 1654171254205,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "us-XVLdD3u34",
    "outputId": "e87dacae-17d2-48db-aeef-7739f9aacd4b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "# Use a GPU if that is possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the base TROCR model\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:44:44.996160Z",
     "iopub.status.busy": "2022-06-03T06:44:44.994297Z",
     "iopub.status.idle": "2022-06-03T06:44:45.002468Z",
     "shell.execute_reply": "2022-06-03T06:44:45.001549Z",
     "shell.execute_reply.started": "2022-06-03T06:44:44.996118Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1654171254202,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "QTGks1Uf4CeB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "# Set the vocab size\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# Set beam search para\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:44:45.004441Z",
     "iopub.status.busy": "2022-06-03T06:44:45.003897Z",
     "iopub.status.idle": "2022-06-03T06:44:46.306265Z",
     "shell.execute_reply": "2022-06-03T06:44:46.305473Z",
     "shell.execute_reply.started": "2022-06-03T06:44:45.004399Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1654171254203,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "B2181gYd4Gvg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# Use the character error rate for evaluation\n",
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:44:46.308372Z",
     "iopub.status.busy": "2022-06-03T06:44:46.307605Z",
     "iopub.status.idle": "2022-06-03T06:44:46.314661Z",
     "shell.execute_reply": "2022-06-03T06:44:46.313815Z",
     "shell.execute_reply.started": "2022-06-03T06:44:46.308335Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1654171254204,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "YOT8qlAW4Jr5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compute the character error rate based on text predictions and actual text\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:44:46.316859Z",
     "iopub.status.busy": "2022-06-03T06:44:46.316096Z",
     "iopub.status.idle": "2022-06-03T06:44:46.328816Z",
     "shell.execute_reply": "2022-06-03T06:44:46.327971Z",
     "shell.execute_reply.started": "2022-06-03T06:44:46.316822Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1654171254204,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "_NWhhGfbQKEL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Empty the GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T06:44:46.333740Z",
     "iopub.status.busy": "2022-06-03T06:44:46.331968Z",
     "iopub.status.idle": "2022-06-03T06:58:00.723921Z",
     "shell.execute_reply": "2022-06-03T06:58:00.722891Z",
     "shell.execute_reply.started": "2022-06-03T06:44:46.333703Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "error",
     "timestamp": 1654170631024,
     "user": {
      "displayName": "I. Kamsteeg",
      "userId": "15945431086150300738"
     },
     "user_tz": -120
    },
    "id": "2zSRTeU04L5F",
    "outputId": "92a8b933-9a6a-414b-aad7-7f5e779f7d59",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Use an AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Loop over the IAM dataset multiple times\n",
    "for epoch in range(10):\n",
    "   # Train the model\n",
    "   model.train()\n",
    "   train_loss = 0.0\n",
    "   # Go through the IAM data\n",
    "   for batch in tqdm(train_dataloader):\n",
    "      for k,v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "      # Do the forward and backward pass\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "        \n",
    "      # Update the loss\n",
    "      train_loss += loss.item()\n",
    "\n",
    "   print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "    \n",
    "   # Evaluate the model with validation\n",
    "   model.eval()\n",
    "   valid_cer = 0.0\n",
    "   with torch.no_grad():\n",
    "     for batch in tqdm(eval_dataloader):\n",
    "       outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "       # Compute the character error rate\n",
    "       cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "       valid_cer += cer \n",
    "\n",
    "   print(\"Validation CER:\", valid_cer / len(eval_dataloader))\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T07:07:01.283743Z",
     "iopub.status.busy": "2022-06-03T07:07:01.283339Z",
     "iopub.status.idle": "2022-06-03T07:07:11.777291Z",
     "shell.execute_reply": "2022-06-03T07:07:11.776507Z",
     "shell.execute_reply.started": "2022-06-03T07:07:01.283709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Use the user input as the path to the test data\n",
    "test_path = sys.argv[1:]\n",
    "# UNCOMMENT the following and change the test path if that works better\n",
    "# test_path = \"../some_path/some_more_path/\"\n",
    "\n",
    "# Create a dataframe for the test data text (unlabeled)\n",
    "filenames = os.listdir(test_path)\n",
    "texts = [\"\"] * 5\n",
    "filenames_df = pd.DataFrame(filenames)\n",
    "texts_df = pd.DataFrame(texts)\n",
    "test_txt_df = pd.concat([filenames_df, texts_df], axis=1)\n",
    "test_txt_df = test_txt_df.set_axis(['file_name', 'text'], axis=1)\n",
    "\n",
    "# Load the trained model\n",
    "model_2 = VisionEncoderDecoderModel.from_pretrained(\"./\")\n",
    "model_2.to(device)\n",
    "\n",
    "# Use the IAMDataset class to save the test data\n",
    "test_dataset = IAMDataset(root_dir=test_path,\n",
    "                           df=test_txt_df,\n",
    "                           processor=processor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "# Go through the test data and predict the handwritten text\n",
    "for batch in tqdm(test_dataloader):\n",
    "    \n",
    "    # Use the model to predict\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    outputs = model.generate(pixel_values)\n",
    "\n",
    "    # Decode the prediction\n",
    "    pred_str = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    # Save the predicted text\n",
    "    text_filename = test_txt_df.iloc[[cnt]]\n",
    "    text_filename = text_filename.iloc[0][\"file_name\"]\n",
    "    text_filename = text_filename.replace('.png', '.txt')\n",
    "    cnt = cnt + 1\n",
    "    with open(text_filename, 'w') as f:\n",
    "        f.write(pred_str[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
